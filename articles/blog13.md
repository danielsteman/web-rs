% id: 13
% title: PyPI over notebooks
% date: 2024-03-08
% tags: data

## The Databricks ecosystem

Recently I have been working a lot with Databricks and quite honestly, wasn't very content with the developer experience. All interaction takes place in the browser, in Databricks-flavoured [Jupyter notebooks](https://jupyter.org/) that make it easy to collaborate in real-time (like with Google docs). This seems to ignore software engineering best practices. First of all, it won't let you use your developer workflow that you've been optimising over the past years, as these notebooks are not integrated development environment (IDE) agnostic. Think of the ability to view a definition of a function, automatic format-on-save, or boiler plate code generation and other magic that intellisense performs. Second, Databricks has a git integration but doesn't support pre-commit hooks. I believe it doesn't matter how experienced you are, pre-commit will catch tiny errors and prevent them from being pushed. You can still perform these checks in the continuous integration (CI) pipeline, but this will increase the feedback loop significantly. Third, interaction is slow. I'm grateful for the performance that Firefox delivers but let's agree that you're not supposed to use it as an IDE. Yes, I'm looking at you [VS Code Web](https://code.visualstudio.com/docs/editor/vscode-web). At last, feedback-loops associated with development in Databricks are usually long because tasks are performed on remote compute. However, this is not inherently a limitation of Databricks but rather an unavoidable trait when dealing with large datasets that don't fit in memory of your own machine. As an engineer I like to think about solutions and so, I tried out a couple of things.

Before continuing, I must say that Databricks is a great product to get started with a datalake. Aside from the managed [Spark](https://spark.apache.org/docs/latest/api/python/index.html) runtime that runs notebooks that have been developed in the browser, it also comes with a [workflow orchestrator](https://docs.databricks.com/en/workflows/index.html), to run notebooks on a schedule. Such a notebook may train a machine learning model that can be tracked with the [MLflow integration](https://mlflow.org/), for example. And then there is the other [fancy stuff](https://docs.databricks.com/en/getting-started/concepts.html) Databricks advocates to ~~up the bill~~ help you become data-driven.

## PyPI

Most people know Python Package Index (PyPI) as a public repository of packages that you can install with `pip`. It is, however, possible to [host your own PyPI repository](https://packaging.python.org/en/latest/guides/hosting-your-own-index/) and lucky for us, most of the Git hosting platforms offer this as a managed solution. Packaging a Python project requires some [configuration](https://packaging.python.org/en/latest/tutorials/packaging-projects/), which can be largely generated by dependency managers such as [Poetry](https://python-poetry.org/). When using Poetry, the configuration defined in `pyproject.toml` looks something like this.

```toml
[tool.poetry]
name = "awesomepackage"
version = "0.0.1"
description = ""
authors = ["Daniel Steman <me@danielsteman.com>"]
readme = "README.md"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

Using a Gitlab personal access token (PAT), it's possible to build and publish this package from your local machine. But the preferred channel is the continuous delivery (CD) pipeline, that runs only after new code is merged into your `(main|master)` branch. A simple pipeline that does this looks like this.

```yaml
stages:
  - publish

.install_python_dependencies-for-publishing:
  before_script:
    - echo "Installing Python dependencies..."
    - pip install poetry
    - poetry install

publish:
  image: python:3.10
  stage: publish
  script:
    - pip install build twine
    - python -m build
    - TWINE_PASSWORD=${CI_JOB_TOKEN} TWINE_USERNAME=gitlab-ci-token python -m twine upload --repository-url ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/pypi dist/*
  rules:
    - if: "$CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH"
      when: on_success
```

What I really like about Gitlab pipelines is that quite a few variables (everything prefixed with `$`) are predefined and can be used to authenticate with a private PyPI repository. It's also possible to abstract repetitive YAML in templates and use it in several repositories. In practice, you might not want to publish the package right away but perform linting and automated testing before. When the package is published, you'll be able to find it in the private package registry of the project. This is different for other Git hosting platforms such as Azure DevOps, where you'll have a central repository for packages published by different projects (or repositories, I use those terms indifferently in this context).

## Using private packages in notebooks

Databricks notebooks can become bloated very quickly. The interactive nature of a notebook doesn't enforce separating code in files or modules as you can run each cell in a notebook independently. This is also an advantage for quick prototyping. For code that runs in production, not so much. A way to minimize the lines of code in a notebook, is to out source as much logic to a private PyPI package. For example, I recently refactored a notebook that fetches data from a third-party API. I contained all logic that sends requests, handles errors, pagination and more, in a package that I install and import in the Databricks runtime.
